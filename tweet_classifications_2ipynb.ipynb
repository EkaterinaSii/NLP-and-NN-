{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliar Functions and Dependencies ⚠️\n",
        "#@markdown ⚡ Run This cell to load the functions required for the exam, as well as all the dependencies and external libraries used in the process.\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Tweet Sample Dataset\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "# POS Tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "# DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "# Math\n",
        "import math\n",
        "\n",
        "# Interactive Widgets\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual, FloatSlider, Layout\n",
        "\n",
        "#Model Selection and Validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
        "\n",
        "def printTokensInVocabs(tokens):\n",
        "  counters = {'CountVectorizer': tfCounter,'TF Normalized':tfNormalizedCounter,'TfIdfVectorizer':tfIdfCounter}\n",
        "  for counterName in counters:\n",
        "    counter = counters[counterName]\n",
        "    newTokens = []\n",
        "    for token in tokenizedTweet:\n",
        "      if token in tfIdfCounter.vocabulary_:\n",
        "        newTokens.append(token)\n",
        "    print(f'Tokens in the Vocabulary of {counterName}: \\t{newTokens}')"
      ],
      "metadata": {
        "id": "J3jYaABABDMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efd1741-5c95-4d34-df4b-fac366fb2490",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples \n",
        "\n",
        "positiveTweets = twitter_samples.strings('positive_tweets.json')\n",
        "negativeTweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "allTweets = []\n",
        "allTweets.extend(positiveTweets)\n",
        "allTweets.extend(negativeTweets)\n",
        "\n",
        "nPositive = len(positiveTweets)\n",
        "nNegative = len(negativeTweets)\n",
        "\n",
        "positiveLabels = np.ones(nPositive)\n",
        "negativeLabels = np.zeros(nNegative)\n",
        "\n",
        "allLabels = []\n",
        "allLabels.extend(positiveLabels)\n",
        "allLabels.extend(negativeLabels)\n",
        "allLabels = np.array(allLabels)"
      ],
      "metadata": {
        "id": "Y5jZBjGujBu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessTweet(tweet):\n",
        "  tweet = re.sub('http[s]?://[\\S]+', ' ', tweet)              # Remove URLs\n",
        "  tweet = re.sub('[\\w]+([._-]\\w+)*@\\w+([.]\\w+)*', ' ', tweet) # Remove e-mails\n",
        "  tweet = re.sub('@\\S+','', tweet)                            # Remove mentions\n",
        "  tweet = re.sub('\\s+', ' ', tweet)                           # Replace repeated spaces to 1 single space\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "0gqh6Ni9kQo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanTokens(tokens):\n",
        "  newTokens = []\n",
        "  for token in tokens:\n",
        "    token = token.lower()\n",
        "    if re.match('^[_*#!$@<=^`>%&\\'\\\"/()\\[\\]\\-+,.:;?]$', token): # Remove tokens that are 1 single punctuation\n",
        "      continue  \n",
        "    if re.match('\\d+', token): # Remove Numbers\n",
        "      continue\n",
        "    if re.match('#[\\w\\d]+', token): # Remove Hashtag\n",
        "      token = token[1:]\n",
        "    newTokens.append(token)\n",
        "  return newTokens"
      ],
      "metadata": {
        "id": "6PivHCXgkay5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTokens(tokens):\n",
        "  splitPattern = r'(?<=[a-z])(?=[A-Z])'\n",
        "  newTokens = []\n",
        "  for token in tokens:\n",
        "    pieces = re.split(splitPattern, token)\n",
        "    newTokens.extend(pieces)\n",
        "  return newTokens"
      ],
      "metadata": {
        "id": "aOOrchL6kdLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def tokenizeTweet(tweet):\n",
        "  tokens = TweetTokenizer().tokenize(tweet)\n",
        "  splittedTokens = splitTokens(tokens)\n",
        "  cleanedTokens = cleanTokens(splittedTokens)\n",
        "  return cleanedTokens"
      ],
      "metadata": {
        "id": "czJGvxEokfoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "englishStopWords = stopwords.words('english')"
      ],
      "metadata": {
        "id": "PgzCVd6YkmV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "8JhiboDImWGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildVectorizers(max_features):\n",
        "  # Term Frequency\n",
        "  tfCounter = CountVectorizer(\n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = englishStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = max_features,\n",
        "  )\n",
        "  tfRepresentation = tfCounter.fit_transform(allTweets)\n",
        "\n",
        "  # TF Normalized\n",
        "  tfNormalizedCounter = TfidfVectorizer(\n",
        "    use_idf = False, norm = 'l2', \n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = englishStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = max_features,\n",
        "  )\n",
        "  tfNormalizedRepresentation = tfNormalizedCounter.fit_transform(allTweets)\n",
        "\n",
        "  # TF-IDF Normalized\n",
        "  tfIdfCounter = TfidfVectorizer(\n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = englishStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = max_features,\n",
        "  )\n",
        "\n",
        "  tfIdfRepresentation = tfIdfCounter.fit_transform(allTweets)\n",
        "\n",
        "  return tfRepresentation, tfNormalizedRepresentation, tfIdfRepresentation, tfCounter, tfNormalizedCounter, tfIdfCounter"
      ],
      "metadata": {
        "id": "1s8TO4xHkq6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ☑️ Pre-Task 1.4: Train Function (0 Points)"
      ],
      "metadata": {
        "id": "ShEJkK-fdzJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndEvaluate(tweets, labels):\n",
        "  # Split Dataset in Train and Test\n",
        "  X_train, X_test, y_train, y_test = train_test_split(tweets, labels, shuffle=True, random_state=10)\n",
        "  \n",
        "  # Build and Train the Model\n",
        "  model = LogisticRegressionCV(max_iter=2000)\n",
        "  model.fit(X_train,y_train)\n",
        "  \n",
        "  # Calculate Accuracy\n",
        "  trainAcc = model.score(X_train, y_train)\n",
        "  print(f'Train Accuracy: {trainAcc*100:.2f}%')\n",
        "  testAcc = model.score(X_test, y_test)\n",
        "  print(f'Test Accuracy: {testAcc*100:.2f}%\\n')\n",
        "  \n",
        "  # Calculate other metrics\n",
        "  tn, fp, fn, tp = confusion_matrix(labels, model.predict(tweets)).ravel()\n",
        "  precision = tp / (tp + fp)\n",
        "  sensitivity = tp / (tp + fn)\n",
        "  specificity = tn / (tn + fp)\n",
        "  print(f'Precision: {precision*100:.2f}%')\n",
        "  print(f'Sensitivity: {sensitivity*100:.2f}%')\n",
        "  print(f'Specificity: {specificity*100:.2f}%')\n",
        "\n",
        "  # Return Variables\n",
        "  results = {\n",
        "      'model': model,\n",
        "      'testAcc':testAcc,\n",
        "      'trainAcc':trainAcc,\n",
        "      'precision':precision,\n",
        "      'sensitivity':sensitivity,\n",
        "      'specificity':specificity\n",
        "  }\n",
        "  return results"
      ],
      "metadata": {
        "id": "kqVY_bTDnY4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "hBgqeSgJBAPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_features = 2000\n",
        "\n",
        "\n",
        "tfRepresentation, tfNormalizedRepresentation, tfIdfRepresentation, tfCounter, tfNormalizedCounter, tfIdfCounter = buildVectorizers(max_features)\n",
        "print('\\n')\n",
        "print('Model (1) Absolute Term Frequency (Word Count)\\n')\n",
        "tfResults = trainAndEvaluate(tfRepresentation, allLabels)\n",
        "print('\\n')\n",
        "print('Model (2) Normalized Term Frequency (TF-IDF with IDF disabled)\\n')\n",
        "tfNormResults = trainAndEvaluate(tfNormalizedRepresentation, allLabels)\n",
        "print('\\n')\n",
        "print('Model (3) TF-IDF Representation\\n')\n",
        "tfIdfResults = trainAndEvaluate(tfIdfRepresentation, allLabels)\n",
        "\n",
        "pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCnbFCfRBrup",
        "outputId": "dd7c0d35-7589-498a-e291-4ee2fe143c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Model (1) Absolute Term Frequency (Word Count)\n",
            "\n",
            "Train Accuracy: 99.99%\n",
            "Test Accuracy: 99.48%\n",
            "\n",
            "Precision: 99.74%\n",
            "Sensitivity: 99.98%\n",
            "Specificity: 99.74%\n",
            "\n",
            "\n",
            "Model (2) Normalized Term Frequency (TF-IDF with IDF disabled)\n",
            "\n",
            "Train Accuracy: 99.99%\n",
            "Test Accuracy: 99.64%\n",
            "\n",
            "Precision: 99.82%\n",
            "Sensitivity: 99.98%\n",
            "Specificity: 99.82%\n",
            "\n",
            "\n",
            "Model (3) TF-IDF Representation\n",
            "\n",
            "Train Accuracy: 99.95%\n",
            "Test Accuracy: 99.80%\n",
            "\n",
            "Precision: 99.86%\n",
            "Sensitivity: 99.96%\n",
            "Specificity: 99.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trainAndSeeErrors(model, tweets, labels):\n",
        "  matrix = tweets.toarray()\n",
        "  for i in range(len(labels)):\n",
        "    pred = model.predict([matrix[i]])\n",
        "    real = labels[i]\n",
        "    if pred != real:\n",
        "      print('---------')\n",
        "      if real == 1: label = '(+)'\n",
        "      else: label = '(-)'\n",
        "      print(f'{label} -> {allTweets[i]}')"
      ],
      "metadata": {
        "id": "0U8gXt01vGj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainAndSeeErrors(tfResults['model'], tfRepresentation, allLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k3NLmqdvGVE",
        "outputId": "6b1a4212-0c1f-47c0-e07b-e38f36d5f780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "(+) -> @ellekagaoan @chinmarquez Catch up once in a while :( &gt;:D&lt; @aditriphosphate @ErinMonzon\n",
            "---------\n",
            "(-) -> all time looww(:(\n",
            "---------\n",
            "(-) -> hugs baek tight : (\n",
            "---------\n",
            "(-) -> @wtfxmbs AMBS please it's harry's jeans :)):):):(\n",
            "---------\n",
            "(-) -> laomma design; Kebaya &amp; Wedding Dress. Bandung - Indonesia\n",
            "LINE: laomma, \n",
            "7DF89150\n",
            "WHATSAPP : (+62) 089624641747\n",
            "Instagram : Laomma_Couture\n",
            "---------\n",
            "(-) -> @c_tuilagi Anytime Lil Nigga!! (: (:\n",
            "---------\n",
            "(-) -> i pOPPED CONFETTI THOUGH ! ! : ( https://t.co/Y79gPDxTIE\n",
            "---------\n",
            "(-) -> Zehr khany ka time is coming soon.....: (\n",
            "---------\n",
            "(-) -> Annnd, now not going to Winchester {:-(\n",
            "---------\n",
            "(-) -> pats jay : (\n",
            "---------\n",
            "(-) -> @bae_ts WHATEVER STIL L YOUNG &gt;:-(\n",
            "---------\n",
            "(-) -> the internet is being a total bitch : (\n",
            "---------\n",
            "(-) -> my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "---------\n",
            "(-) -> @CHEDA_KHAN Thats life. I get calls from people I havent seen in 20 years and its always favours : (\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainAndSeeErrors(tfNormResults['model'], tfNormalizedRepresentation, allLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7OdPZ9avzjQ",
        "outputId": "d11c4706-75e8-4208-8cb1-cf2c34e7111e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "(+) -> @ellekagaoan @chinmarquez Catch up once in a while :( &gt;:D&lt; @aditriphosphate @ErinMonzon\n",
            "---------\n",
            "(-) -> all time looww(:(\n",
            "---------\n",
            "(-) -> stu is mean, i just wanna sleep : (\n",
            "---------\n",
            "(-) -> @c_tuilagi Anytime Lil Nigga!! (: (:\n",
            "---------\n",
            "(-) -> i pOPPED CONFETTI THOUGH ! ! : ( https://t.co/Y79gPDxTIE\n",
            "---------\n",
            "(-) -> Zehr khany ka time is coming soon.....: (\n",
            "---------\n",
            "(-) -> Annnd, now not going to Winchester {:-(\n",
            "---------\n",
            "(-) -> my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "---------\n",
            "(-) -> @CHEDA_KHAN Thats life. I get calls from people I havent seen in 20 years and its always favours : (\n",
            "---------\n",
            "(-) -> Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainAndSeeErrors(tfIdfResults['model'], tfIdfRepresentation, allLabels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FciNHdG8vzUE",
        "outputId": "4986d8c2-cf04-40f0-9f2c-2cbc49df013b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------\n",
            "(+) -> Remember that one time I didn't go to flume/kaytranada/alunageorge even though I had tickets? I still want to kms. : ) : )\n",
            "---------\n",
            "(+) -> @ellekagaoan @chinmarquez Catch up once in a while :( &gt;:D&lt; @aditriphosphate @ErinMonzon\n",
            "---------\n",
            "(-) -> @Israelgirly They sure do, esp now when ppl are talking crap about Millie!! &gt;:( I'll go straight to that FB page:)\n",
            "---------\n",
            "(-) -> @wtfxmbs AMBS please it's harry's jeans :)):):):(\n",
            "---------\n",
            "(-) -> @c_tuilagi Anytime Lil Nigga!! (: (:\n",
            "---------\n",
            "(-) -> i pOPPED CONFETTI THOUGH ! ! : ( https://t.co/Y79gPDxTIE\n",
            "---------\n",
            "(-) -> Annnd, now not going to Winchester {:-(\n",
            "---------\n",
            "(-) -> my beloved grandmother : ( https://t.co/wt4oXq5xCf\n",
            "---------\n",
            "(-) -> Sr. Financial Analyst - Expedia, Inc.: (#Bellevue, WA) http://t.co/ktknMhvwCI #Finance #ExpediaJobs #Job #Jobs #Hiring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myTestTweet = '''Annnd, now not going to Winchester {:-('''\n",
        "myTestTweet = myTestTweet.replace('\\n',' ')\n",
        "preprocessedTweet = preprocessTweet(myTestTweet)\n",
        "tokenizedTweet = tokenizeTweet(preprocessedTweet)\n",
        "\n",
        "print(f'Original Tweet:\\t\\t{myTestTweet}')\n",
        "print(f'Pre-processed Tweet:\\t{preprocessedTweet}')\n",
        "print(f'Tokenized Tweet:\\t{tokenizedTweet}')\n",
        "printTokensInVocabs(tokenizedTweet)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT7NHGb0wj9y",
        "outputId": "43fcaefe-d0fe-4d9e-eba2-62567f3443c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tweet:\t\tAnnnd, now not going to Winchester {:-(\n",
            "Pre-processed Tweet:\tAnnnd, now not going to Winchester {:-(\n",
            "Tokenized Tweet:\t['annnd', 'now', 'not', 'going', 'to', 'winchester', '{:']\n",
            "Tokens in the Vocabulary of CountVectorizer: \t['going']\n",
            "Tokens in the Vocabulary of TF Normalized: \t['going']\n",
            "Tokens in the Vocabulary of TfIdfVectorizer: \t['going']\n"
          ]
        }
      ]
    }
  ]
}