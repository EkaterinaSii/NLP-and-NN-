{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Tweet Sample Dataset\n",
        "nltk.download('twitter_samples')\n",
        "\n",
        "# POS Tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Lemmatizer \n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "# Stop Words\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Numpy\n",
        "import numpy as np\n",
        "\n",
        "# Regular Expressions\n",
        "import re\n",
        "\n",
        "# DataFrames\n",
        "import pandas as pd\n",
        "\n"
      ],
      "metadata": {
        "id": "MlZGiaOBVbm7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4088b33b-c132-41eb-a9b8-dc2d05e33b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Auxiliar Functions for Evaluation ⚠️\n",
        "#@markdown ⚡ Run This cell to load the functions that help you to validate if your tasks are correctly done or not\n",
        "\n",
        "\n",
        "############    Validate equivalence    ############\n",
        "\n",
        "def listsHaveSameValues(list1, list2):\n",
        "    if type(list1) != list or type(list2) != list:\n",
        "        return False\n",
        "    if len(list1) != len(list2):\n",
        "        return False\n",
        "    for item in list1:\n",
        "        if item not in list2:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def dictionariesHaveSameValues(dict1, dict2):\n",
        "    if type(dict1) != dict or type(dict2) != dict:\n",
        "        return False\n",
        "    if len(dict1) != len(dict2):\n",
        "        return False\n",
        "    for key in dict1:\n",
        "        if key not in dict2:\n",
        "            return False\n",
        "        if dict1[key] != dict2[key]:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def stringsHaveSameValues(str1, str2):\n",
        "    return str1 == str2\n",
        "\n",
        "############    Answer is Correct    ############\n",
        "\n",
        "def answerIsCorrectList(correctAnswer, input, yourFunction):\n",
        "  import types\n",
        "  if not isinstance(yourFunction, types.FunctionType):\n",
        "    return False\n",
        "  yourAnswer = yourFunction(input)\n",
        "  return listsHaveSameValues(correctAnswer, yourAnswer)\n",
        "\n",
        "def answerIsCorrectDict(correctAnswer, input, yourFunction):\n",
        "  import types\n",
        "  if not isinstance(yourFunction, types.FunctionType):\n",
        "    return False\n",
        "  yourAnswer = yourFunction(input)\n",
        "  return dictionariesHaveSameValues(correctAnswer, yourAnswer)\n",
        "\n",
        "def answerIsCorrectString(correctAnswer, input, yourFunction):\n",
        "  import types\n",
        "  if not isinstance(yourFunction, types.FunctionType):\n",
        "    return False\n",
        "  yourAnswer = yourFunction(input)\n",
        "  return stringsHaveSameValues(correctAnswer, yourAnswer)\n",
        "\n",
        "############    Print Diffs    ############\n",
        "\n",
        "def printDifferences(correctAnswer, yourAnswer, input):\n",
        "      print(f'Input:\\t\\t{input}')\n",
        "      print(f'Correct Answer:\\t{correctAnswer}')\n",
        "      print(f'Your Answer: \\t{yourAnswer}')\n",
        "      print()\n",
        "  \n",
        "\n",
        "def printDifferencesBetweenDicts(correctDict, yourDict, input=None):\n",
        "    keysOnlyInCorrect = []\n",
        "    keysOnlyInYours = []\n",
        "    keysWithDifferentValues = []\n",
        "\n",
        "    allKeys = []\n",
        "    allKeys.extend(list(correctDict))\n",
        "    allKeys.extend(list(yourDict))\n",
        "    allKeys = set(allKeys)\n",
        "\n",
        "    for key in allKeys:\n",
        "      if (key in correctDict) and (key not in yourDict):\n",
        "        keysOnlyInCorrect.append(key)\n",
        "      elif (key in yourDict) and (key not in correctDict):\n",
        "        keysOnlyInYours.append(key)\n",
        "      elif correctDict[key] != yourDict[key]:\n",
        "        keysWithDifferentValues.append(key)\n",
        "    if (input != None):\n",
        "      print(f'Input:\\n{input}\\n')\n",
        "    print(f'Keys that you are missing:\\n{keysOnlyInCorrect}\\n')\n",
        "    print(f'Keys that should not be in your answer:\\n{keysOnlyInYours}\\n')\n",
        "    print(f'Keys with wrong values:\\n{keysWithDifferentValues}')\n",
        "\n",
        "############    Test Answer    ############\n",
        "\n",
        "def testAnswers(yourImplementation, answersAndInputs, answerType):\n",
        "  if answerType == 'list':\n",
        "    answerIsCorrect = answerIsCorrectList\n",
        "    printDiffs = printDifferences\n",
        "  elif answerType == 'dict':\n",
        "    answerIsCorrect = answerIsCorrectDict\n",
        "    printDiffs = printDifferencesBetweenDicts\n",
        "  elif answerType == 'string':\n",
        "    answerIsCorrect = answerIsCorrectString\n",
        "    printDiffs = printDifferences\n",
        "  else:\n",
        "    raise Exception(f'Answer Type is not recognized: {answerType}')\n",
        "  import types\n",
        "  if not isinstance(yourImplementation, types.FunctionType):\n",
        "    raise Exception('Your implementation is not a function')\n",
        "  nTests = len(answersAndInputs)\n",
        "  for i in range(nTests):\n",
        "    correctAnswer, input = answersAndInputs[i]\n",
        "    print(f'Test {i+1}/{nTests} ', end='')\n",
        "    if answerIsCorrect(correctAnswer, input, yourFunction=yourImplementation):\n",
        "      print('✅')\n",
        "      print(f'Input: \\t{input}')\n",
        "      print(f'Answer:\\t{correctAnswer}')\n",
        "      print()\n",
        "    else:\n",
        "      yourAnswer = yourImplementation(input)\n",
        "      print('❌')\n",
        "      printDiffs(correctAnswer, yourAnswer, input)\n",
        "\n",
        "############    Print    ############\n",
        "\n",
        "def showError(message, functionName):\n",
        "    print(f'Error at Function {functionName}: {message}')\n",
        "\n",
        "print('The auxiliar functions were loaded successfully')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "cuJYie4962at",
        "outputId": "64b18b51-568e-4be0-9a8f-9f2c4b636b66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The auxiliar functions were loaded successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transformPosTag(posTag):\n",
        "  tags = {'NOUN':'n','VERB':'v','ADJ':'a','ADV':'r'}\n",
        "  newPosTag = tags.get(posTag)\n",
        "  return newPosTag"
      ],
      "metadata": {
        "id": "W-VmVmKNWiM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "f8BwosQJZ2Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatizeTokens(tokens):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatizedTokens = []\n",
        "  tokensWithPosTags = pos_tag(tokens,tagset='universal')\n",
        "  for token,posTag in tokensWithPosTags:\n",
        "    posTag = transformPosTag(posTag)\n",
        "    if posTag == None:\n",
        "      lemmatizedTokens.append(token)\n",
        "    else:\n",
        "      lemmatizedToken = lemmatizer.lemmatize(token,posTag)\n",
        "  return lemmatizedTokens"
      ],
      "metadata": {
        "id": "6uukdntk_Pgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = 'two women , two oxen , two oases , two mice'.split()\n",
        "print('Tokens:',tokens)\n",
        "print('Lemmatized Tokens:',lemmatizeTokens(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUIDjDsT_EBd",
        "outputId": "75bd16be-f132-402e-9ca4-0adc222aaae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['two', 'women', ',', 'two', 'oxen', ',', 'two', 'oases', ',', 'two', 'mice']\n",
            "Lemmatized Tokens: ['two', 'woman', ',', 'two', 'ox', ',', 'two', 'oasis', ',', 'two', 'mouse']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test your implementation of lemmatizeTokens() ⚠️\n",
        "\n",
        "#@markdown ⚡ Run this cell to validate if you implemented the function correctly\n",
        "\n",
        "def checkLemmatizeTokens():\n",
        "  answersAndInputs = [\n",
        "    # Nouns and Verbs\n",
        "    (['i','be','google','everything'],['i','am','googling','everything']),\n",
        "    # Verbs\n",
        "    (['play', 'with', 'that', 'be', 'very', 'dangerous'],['playing','with','that','is','very','dangerous']),\n",
        "    # Noun\n",
        "    (['animal', 'and', 'pet', 'be', 'beautiful'],['animals', 'and', 'pets', 'are', 'beautiful']),\n",
        "    # Noun\n",
        "    (['thanks', 'for', 'your', 'blessing'],['thanks', 'for', 'your', 'blessings']),\n",
        "    # Comparatives and Superlatives\n",
        "    (['you', 'be', 'smart', 'than', 'average,', 'but', 'not', 'the', 'smart'],['you', 'are', 'smarter', 'than', 'average,', 'but', 'not', 'the', 'smartest']),\n",
        "    # Irregular conjugations\n",
        "    (['two', 'woman', ',', 'two', 'ox', ',', 'two', 'oasis', ',', 'two', 'mouse'],['two', 'women', ',', 'two', 'oxen', ',', 'two', 'oases', ',', 'two', 'mice']\n",
        "),\n",
        "    # Empty Input should work too\n",
        "    ([],[])\n",
        "  ]\n",
        "  yourImplementation = lemmatizeTokens\n",
        "  answerType = 'list'\n",
        "  testAnswers(yourImplementation, answersAndInputs, answerType)\n",
        "\n",
        "checkLemmatizeTokens()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R46gJGAQJmV9",
        "outputId": "770ceab3-7ed8-4602-a343-7ee3e136214f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test 1/7 ✅\n",
            "Input: \t['i', 'am', 'googling', 'everything']\n",
            "Answer:\t['i', 'be', 'google', 'everything']\n",
            "\n",
            "Test 2/7 ✅\n",
            "Input: \t['playing', 'with', 'that', 'is', 'very', 'dangerous']\n",
            "Answer:\t['play', 'with', 'that', 'be', 'very', 'dangerous']\n",
            "\n",
            "Test 3/7 ✅\n",
            "Input: \t['animals', 'and', 'pets', 'are', 'beautiful']\n",
            "Answer:\t['animal', 'and', 'pet', 'be', 'beautiful']\n",
            "\n",
            "Test 4/7 ✅\n",
            "Input: \t['thanks', 'for', 'your', 'blessings']\n",
            "Answer:\t['thanks', 'for', 'your', 'blessing']\n",
            "\n",
            "Test 5/7 ✅\n",
            "Input: \t['you', 'are', 'smarter', 'than', 'average,', 'but', 'not', 'the', 'smartest']\n",
            "Answer:\t['you', 'be', 'smart', 'than', 'average,', 'but', 'not', 'the', 'smart']\n",
            "\n",
            "Test 6/7 ✅\n",
            "Input: \t['two', 'women', ',', 'two', 'oxen', ',', 'two', 'oases', ',', 'two', 'mice']\n",
            "Answer:\t['two', 'woman', ',', 'two', 'ox', ',', 'two', 'oasis', ',', 'two', 'mouse']\n",
            "\n",
            "Test 7/7 ✅\n",
            "Input: \t[]\n",
            "Answer:\t[]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessTweet(tweet):\n",
        "  tweet = re.sub('http[s]?://[\\S]+', ' ', tweet)              # Remove URLs\n",
        "  tweet = re.sub('[\\w]+([._-]\\w+)*@\\w+([.]\\w+)*', ' ', tweet) # Remove e-mails\n",
        "  tweet = re.sub('@\\S+','', tweet)                            # Remove mentions\n",
        "  tweet = re.sub('\\s+', ' ', tweet)                           # Replace repeated spaces to 1 single space\n",
        "  return tweet"
      ],
      "metadata": {
        "id": "RYY61DLc7LMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanTokens(tokens):\n",
        "  newTokens = []\n",
        "  for token in tokens:\n",
        "    token = token.lower()\n",
        "    if re.match('^[_*#!$@<=^`>%&\\'\\\"/()\\[\\]\\-+,.:;?]$', token): # Remove tokens that are 1 single punctuation\n",
        "      continue  \n",
        "    if re.match('\\d+', token): # Remove Numbers\n",
        "      continue\n",
        "    if re.match('#[\\w\\d]+', token): # Remove Hashtag\n",
        "      token = token[1:]\n",
        "    newTokens.append(token)\n",
        "  return newTokens\n"
      ],
      "metadata": {
        "id": "f1nzleT8OwxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def splitTokens(tokens):\n",
        "  splitPattern = r'(?<=[a-z])(?=[A-Z])'\n",
        "  newTokens = []\n",
        "  for token in tokens:\n",
        "    pieces = re.split(splitPattern, token)\n",
        "    newTokens.extend(pieces)\n",
        "  return newTokens"
      ],
      "metadata": {
        "id": "Vz70sikRPZkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def tokenizeTweet(tweet):\n",
        "  tokens = TweetTokenizer().tokenize(tweet)\n",
        "  splittedTokens = splitTokens(tokens)\n",
        "  cleanedTokens = cleanTokens(splittedTokens)\n",
        "  lemmatizedTokens = lemmatizeTokens(cleanedTokens)\n",
        "  return lemmatizedTokens"
      ],
      "metadata": {
        "id": "oAj4QDm57NoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "englishStopWords = stopwords.words('english')"
      ],
      "metadata": {
        "id": "ht3QBdGB_fvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples \n",
        "\n",
        "sampleSet = twitter_samples.strings('positive_tweets.json')"
      ],
      "metadata": {
        "id": "PwHi5HKTSk30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "counter = CountVectorizer(\n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = englishStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = 900,\n",
        "  )"
      ],
      "metadata": {
        "id": "C-B_8XyRLPSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allBagsOfWord = counter.fit_transform(sampleSet)\n",
        "vocab = list(counter.get_feature_names_out())\n",
        "\n",
        "print('\\nIMPORTANT !!!!\\n\\n')\n",
        "print('This is the vocabulary that you will need to read in order to finish Task 3 succesfully:\\n')\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuTHktO0_hTA",
        "outputId": "8ab94467-c4b3-475b-9448-29ce9d258d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IMPORTANT !!!!\n",
            "\n",
            "\n",
            "This is the vocabulary that you will need to read in order to finish Task 3 succesfully:\n",
            "\n",
            "['):', '..', '...', ':)', ':-)', ':/', ':d', ':p', ';)', '<3', '\\\\', 'able', 'absolutely', 'account', 'act', 'actually', 'add', 'address', 'advice', 'af', 'afternoon', 'ago', 'agree', 'ah', 'air', 'al', 'album', 'almost', 'along', 'already', 'alright', 'also', 'always', 'amaze', 'amazing', 'android', 'another', 'answer', 'anyone', 'anything', 'anytime', 'anyway', 'apology', 'app', 'apparently', 'apply', 'appreciate', 'aqui', 'around', 'arrive', 'art', 'article', 'artist', 'ask', 'asleep', 'august', 'available', 'awake', 'away', 'awesome', 'aww', 'awww', 'awwww', 'b', 'babe', 'baby', 'back', 'bad', 'bae', 'bajrangi', 'ball', 'bam', 'bath', 'bc', 'bday', 'beat', 'beautiful', 'beauty', 'become', 'bed', 'believe', 'best', 'bestfriend', 'bet', 'bhaijaan', 'bi0', 'big', 'bill', 'birthday', 'bit', 'bless', 'blog', 'blue', 'body', 'book', 'bore', 'bot', 'boy', 'brain', 'brand', 'break', 'brilliant', 'bring', 'bro', 'btw', 'buddy', 'build', 'bulb', 'business', 'busy', 'button', 'buy', 'bye', 'c', 'cake', 'call', 'camera', \"can't\", 'cant', 'car', 'card', 'care', 'case', 'cat', 'catch', 'cause', 'celebrate', 'ceo', 'ceo1month', 'chance', 'change', 'channel', 'chat', 'check', 'cheer', 'child', 'chill', 'chocolate', 'choice', 'city', 'class', 'click', 'close', 'coffee', 'coin', 'collection', 'colour', 'come', 'comment', 'community', 'complete', 'computer', 'concert', 'congrats', 'congratulation', 'connect', 'contact', 'contest', 'conversation', 'cool', 'copy', 'corn', 'cough', 'could', 'count', 'country', 'couple', 'course', 'cousin', 'cover', 'cream', 'create', 'cry', 'cup', 'cut', 'cute', 'cutie', 'dad', 'dan', 'dark', 'date', 'daughter', 'dave', 'day', 'dear', 'decision', 'definitely', 'deserve', 'design', 'detail', 'die', 'different', 'dinner', 'dm', 'doesnt', 'dog', 'dont', 'dot', 'download', 'draw', 'dream', 'dress', 'drink', 'drive', 'drop', 'dry', 'duck', 'dude', 'e', 'ear', 'early', 'earth', 'easy', 'eat', 'edit', 'either', 'else', 'email', 'employer', 'end', 'engage', 'enjoy', 'enough', 'enter', 'episode', 'even', 'event', 'ever', 'every', 'everyone', 'everything', 'exactly', 'excellent', 'excite', 'expect', 'experience', 'expert', 'extra', 'eye', 'f', 'fab', 'fabulous', 'face', 'facebook', 'fact', 'fair', 'fall', 'family', 'fan', 'fantastic', 'fantasy', 'far', 'fashion', 'fav', 'favorite', 'favourite', 'fb', 'fback', 'feature', 'feedback', 'feel', 'feeling', 'festival', 'ff', 'fight', 'figure', 'fill', 'film', 'final', 'finally', 'find', 'fine', 'finger', 'finish', 'first', 'five', 'fix', 'flipkart', 'flow', 'fly', 'folk', 'follback', 'follow', 'followback', 'follower', 'food', 'forever', 'forget', 'forward', 'four', 'france', 'free', 'freebie', 'french', 'fresh', 'friday', 'friend', 'fruit', 'fuck', 'full', 'fun', 'funny', 'future', 'g', 'gain', 'game', 'garden', 'get', 'gift', 'girl', 'give', 'giveaway', 'glad', 'glass', 'go', 'god', 'gold', 'gonna', 'good', 'goodbye', 'goodmorning', 'goodnight', 'google', 'gorgeous', 'gotta', 'great', 'group', 'grow', 'guess', 'guy', 'h', 'ha', 'haha', 'hahaha', 'hai', 'hair', 'half', 'happen', 'happiness', 'happy', 'hard', 'harry', 'hashtag', 'hate', \"he's\", 'head', 'hear', 'heart', 'hehe', 'hello', 'help', \"here's\", 'hey', 'hi', 'high', 'hit', 'hold', 'holiday', 'home', 'hop', 'hope', 'hopefully', 'hot', 'hotel', 'hour', 'house', 'hug', 'huge', 'human', 'hun', 'hurry', 'hurt', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ice', 'icecream', 'id', 'idea', 'ill', 'ily', 'im', 'image', 'include', 'incredible', 'indeed', 'indiemusic', 'influencer', 'influencers', 'info', 'inside', 'inspire', 'instagram', 'instead', 'interest', 'interested', 'interesting', 'international', 'interview', 'invite', 'island', 'issue', \"it'll\", 'james', 'jesus', 'job', 'john', 'join', 'journey', 'joy', 'july', 'jumma', 'junior', 'k', 'keep', 'khan', 'kid', 'kik', 'kikgirl', 'kikmeboys', 'kiksexting', 'kim', 'kind', 'kinda', 'kitchen', 'knock', 'know', 'kunorifor', 'l', 'la', 'lack', 'lady', 'last', 'late', 'later', 'latin', 'laugh', 'launch', 'lay', 'leader', 'league', 'learn', 'least', 'leave', 'less', 'let', \"let's\", 'liam', 'life', 'light', 'like', 'limit', 'line', 'link', 'list', 'listen', 'literally', 'little', 'live', 'lol', 'london', 'long', 'look', 'lose', 'loser', 'lot', 'love', 'lovely', 'low', 'luck', 'lucky', 'luke', 'lunch', 'luv', 'main', 'make', 'man', 'many', 'mary', 'masaan', 'match', 'mate', 'matt', 'matter', 'may', 'maybe', 'mean', 'medium', 'meet', 'meeting', 'mega', 'member', 'men', 'mention', 'message', 'michael', 'might', 'min', 'mind', 'mine', 'minecraft', 'minute', 'miss', 'mka', 'mm', 'model', 'mom', 'moment', 'monday', 'money', 'month', 'mood', 'morning', 'mother', 'move', 'movie', 'mr', 'mubarak', 'much', 'music', 'must', 'n', 'na', 'name', 'near', 'need', 'never', 'new', 'news', 'next', 'nice', 'night', 'nope', 'note', 'nothing', 'notice', 'number', 'offer', 'office', 'official', 'officially', 'oh', 'ok', 'okay', 'old', 'omg', 'one', 'online', 'oops', 'open', 'oppa', 'opportunity', 'order', 'original', 'others', 'otherwise', 'outside', 'p', 'pack', 'page', 'pain', 'panda', 'parent', 'paris', 'park', 'part', 'party', 'pass', 'past', 'pay', 'people', 'perfect', 'person', 'ph', 'phone', 'photo', 'pic', 'pick', 'picture', 'pin', 'place', 'plan', 'play', 'playlist', 'please', 'pleasse', 'pleasure', 'pls', 'plz', 'point', 'pool', 'pop', 'positive', 'possible', 'post', 'power', 'present', 'pretty', 'pride', 'pro', 'probably', 'problem', 'product', 'program', 'project', 'promise', 'proud', 'put', 'quacketyquack', 'queen', 'question', 'quick', 'quite', 'quote', 'r', 'race', 'radio', 'rain', 'rather', 'read', 'ready', 'real', 'realize', 'really', 'reason', 'receive', 'recent', 'recipe', 'record', 'red', 'regret', 'release', 'remember', 'reply', 'response', 'rest', 'retweet', 'retweeted', 'retweets', 'review', 'right', 'rise', 'road', 'rock', 'roll', 'room', 'rt', 'rts', 'ruin', 'run', 'sa', 'safe', 'saturday', 'save', 'saw', 'say', 'school', 'science', 'scope', 'season', 'second', 'secret', 'see', 'seem', 'selfie', 'selfies', 'send', 'sense', 'service', 'set', 'sex', 'sexy', 'shall', 'share', 'ship', 'shit', 'shoot', 'shop', 'short', 'shot', 'shout', 'show', 'side', 'sign', 'since', 'single', 'sir', 'sister', 'sit', 'site', 'skin', 'sleep', 'smile', 'snapchat', 'someone', 'something', 'sometimes', 'song', 'soon', 'sore', 'sorry', 'sort', 'sound', 'speak', 'special', 'spread', 'stand', 'star', 'start', 'stats', 'stay', 'stick', 'still', 'stop', 'store', 'story', 'stream', 'stress', 'strong', 'student', 'study', 'stuff', 'stupid', 'style', 'subject', 'success', 'suck', 'summer', 'sun', 'sunday', 'sunshine', 'super', 'support', 'suppose', 'sure', 'surprise', 'sweet', 'tag', 'take', 'talk', 'tank', 'tea', 'team', 'teen', 'tell', 'test', 'text', 'tgif', 'thank', 'thanks', 'thankyou', \"that's\", 'thats', \"there's\", \"they'll\", \"they're\", 'thing', 'think', 'thinking', 'tho', 'though', 'thought', 'three', 'throw', 'thx', 'ticket', 'till', 'time', 'today', 'together', 'tom', 'tomorrow', 'tonight', 'top', 'totally', 'touch', 'tour', 'train', 'travel', 'trip', 'true', 'trust', 'try', 'tuesday', 'tune', 'turn', 'tweet', 'twitch', 'twitter', 'two', 'ty', 'type', 'u', 'understand', 'unfollowers', 'update', 'ur', 'us', 'use', 'v', 'vacation', 'version', 'via', 'vid', 'video', 'view', 'visit', 'vote', 'w', 'wait', 'wake', 'walk', 'wanna', 'want', 'warm', 'warsaw', 'waste', 'watch', 'water', 'way', \"we'll\", \"we're\", \"we've\", 'wear', 'weather', 'website', 'week', 'week1', 'weekend', 'weight', 'welcome', 'well', \"what's\", 'whats', \"who's\", 'whole', 'whoop', 'wicked', 'win', 'window', 'wish', 'without', 'wolf', 'wonder', 'wonderful', 'word', 'work', 'workout', 'world', 'worry', 'worth', 'would', 'wow', 'write', 'wrong', 'wsale', 'x', 'xx', 'xxx', 'ya', 'yay', 'yeah', 'year', 'yep', 'yes', 'yesterday', 'yet', 'yo', 'young', 'youth', 'youtube', 'yup', '|', '~', '£', '–', '—', '’', '“', '”', '…', '←', '─', '♡', '♥', '✧', '❤', '️', '🍸', '👈', '👉', '💕', '💙', '💚', '💜', '😁', '😂', '😊', '😘']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allStopWords = []\n",
        "allStopWords.extend(englishStopWords)"
      ],
      "metadata": {
        "id": "fJaJex917RdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import twitter_samples \n",
        "\n",
        "positiveTweets = twitter_samples.strings('positive_tweets.json')\n",
        "negativeTweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "sU1OfVCJMJaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listAllTweets = []\n",
        "listAllTweets.extend(positiveTweets)\n",
        "listAllTweets.extend(negativeTweets)"
      ],
      "metadata": {
        "id": "XSGt-FHALILV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "counter = CountVectorizer(\n",
        "    preprocessor = preprocessTweet,\n",
        "    stop_words = allStopWords,\n",
        "    tokenizer = tokenizeTweet,\n",
        "    max_features = 840,\n",
        "  )"
      ],
      "metadata": {
        "id": "SFMbKeuMVCF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allTweets = counter.fit_transform(listAllTweets)"
      ],
      "metadata": {
        "id": "8jJ6zyy0TDJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2800c7-e82c-42b8-9cf7-7b59298ddfd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ceo', 'far'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sizePositive = len(positiveTweets)\n",
        "sizeNegative = len(negativeTweets)\n",
        "\n",
        "positiveLabels = np.ones(sizePositive)\n",
        "negativeLabels = np.zeros(sizeNegative)\n",
        "allLabels = np.hstack((positiveLabels,negativeLabels))"
      ],
      "metadata": {
        "id": "6V1BONkaLgaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(allTweets, allLabels, shuffle=True)"
      ],
      "metadata": {
        "id": "6TyraegFK8Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV"
      ],
      "metadata": {
        "id": "j9Z9CyndK_9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FvrzxKK9Nocr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegressionCV(max_iter=2000)\n",
        "model.fit(X_train,y_train)\n",
        "print('The training of the Logistic Regression Model has finished :)')"
      ],
      "metadata": {
        "id": "THUXG1jsPsin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499321b7-c5a8-414b-bed2-2301b4f4679d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training of the Logistic Regression Model has finished :)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Run this cell to get the accuracy in both datasets (Train and Test) using the threshold\n",
        "\n",
        "threshold =   0.71#@param {type:\"number\"}\n",
        "\n",
        "def getPrediction(probabilities, threshold):\n",
        "  predictions = []\n",
        "  for prob in probabilities:\n",
        "    if prob >= threshold:\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = 0\n",
        "    predictions.append(pred)\n",
        "  return predictions\n",
        "\n",
        "testProbabilities = model.predict_proba(X_test)[:,1]\n",
        "testPredictions = getPrediction(testProbabilities, threshold)\n",
        "\n",
        "trainProbabilities = model.predict_proba(X_train)[:,1]\n",
        "trainPredictions = getPrediction(trainProbabilities, threshold)\n",
        "\n",
        "import sklearn\n",
        "\n",
        "trainAccuracy = sklearn.metrics.accuracy_score(y_train, trainPredictions)\n",
        "testAccuracy = sklearn.metrics.accuracy_score(y_test, testPredictions)\n",
        "\n",
        "trainAccuracy *= 100\n",
        "testAccuracy *= 100\n",
        "\n",
        "print(f'Train Accuracy: {trainAccuracy:.2f}%')\n",
        "print(f'Test Accuracy: {testAccuracy:.2f}%')"
      ],
      "metadata": {
        "id": "sIo5FRX4eKKA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34178b51-4388-4722-dc78-12a8a1816c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 99.93%\n",
            "Test Accuracy: 99.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(f'\\t((  Classification Report for Test Set  ))\\n')\n",
        "\n",
        "y_test_pred = model.predict(X_test)\n",
        "print(classification_report(y_true=y_test , y_pred=y_test_pred))"
      ],
      "metadata": {
        "id": "Uhn1Yrb2U3jA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add83bc3-21e1-4ded-d933-94dc54f84ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t((  Classification Report for Test Set  ))\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.99      1.00      1259\n",
            "         1.0       0.99      1.00      1.00      1241\n",
            "\n",
            "    accuracy                           1.00      2500\n",
            "   macro avg       1.00      1.00      1.00      2500\n",
            "weighted avg       1.00      1.00      1.00      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab)"
      ],
      "metadata": {
        "id": "21OcFlRIVr2r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37c80cac-d4bd-4cc4-fa8b-60e8bcf93766"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['):', '..', '...', ':)', ':-)', ':/', ':d', ':p', ';)', '<3', '\\\\', 'able', 'absolutely', 'account', 'act', 'actually', 'add', 'address', 'advice', 'af', 'afternoon', 'ago', 'agree', 'ah', 'air', 'al', 'album', 'almost', 'along', 'already', 'alright', 'also', 'always', 'amaze', 'amazing', 'android', 'another', 'answer', 'anyone', 'anything', 'anytime', 'anyway', 'apology', 'app', 'apparently', 'apply', 'appreciate', 'aqui', 'around', 'arrive', 'art', 'article', 'artist', 'ask', 'asleep', 'august', 'available', 'awake', 'away', 'awesome', 'aww', 'awww', 'awwww', 'b', 'babe', 'baby', 'back', 'bad', 'bae', 'bajrangi', 'ball', 'bam', 'bath', 'bc', 'bday', 'beat', 'beautiful', 'beauty', 'become', 'bed', 'believe', 'best', 'bestfriend', 'bet', 'bhaijaan', 'bi0', 'big', 'bill', 'birthday', 'bit', 'bless', 'blog', 'blue', 'body', 'book', 'bore', 'bot', 'boy', 'brain', 'brand', 'break', 'brilliant', 'bring', 'bro', 'btw', 'buddy', 'build', 'bulb', 'business', 'busy', 'button', 'buy', 'bye', 'c', 'cake', 'call', 'camera', \"can't\", 'cant', 'car', 'card', 'care', 'case', 'cat', 'catch', 'cause', 'celebrate', 'ceo', 'ceo1month', 'chance', 'change', 'channel', 'chat', 'check', 'cheer', 'child', 'chill', 'chocolate', 'choice', 'city', 'class', 'click', 'close', 'coffee', 'coin', 'collection', 'colour', 'come', 'comment', 'community', 'complete', 'computer', 'concert', 'congrats', 'congratulation', 'connect', 'contact', 'contest', 'conversation', 'cool', 'copy', 'corn', 'cough', 'could', 'count', 'country', 'couple', 'course', 'cousin', 'cover', 'cream', 'create', 'cry', 'cup', 'cut', 'cute', 'cutie', 'dad', 'dan', 'dark', 'date', 'daughter', 'dave', 'day', 'dear', 'decision', 'definitely', 'deserve', 'design', 'detail', 'die', 'different', 'dinner', 'dm', 'doesnt', 'dog', 'dont', 'dot', 'download', 'draw', 'dream', 'dress', 'drink', 'drive', 'drop', 'dry', 'duck', 'dude', 'e', 'ear', 'early', 'earth', 'easy', 'eat', 'edit', 'either', 'else', 'email', 'employer', 'end', 'engage', 'enjoy', 'enough', 'enter', 'episode', 'even', 'event', 'ever', 'every', 'everyone', 'everything', 'exactly', 'excellent', 'excite', 'expect', 'experience', 'expert', 'extra', 'eye', 'f', 'fab', 'fabulous', 'face', 'facebook', 'fact', 'fair', 'fall', 'family', 'fan', 'fantastic', 'fantasy', 'far', 'fashion', 'fav', 'favorite', 'favourite', 'fb', 'fback', 'feature', 'feedback', 'feel', 'feeling', 'festival', 'ff', 'fight', 'figure', 'fill', 'film', 'final', 'finally', 'find', 'fine', 'finger', 'finish', 'first', 'five', 'fix', 'flipkart', 'flow', 'fly', 'folk', 'follback', 'follow', 'followback', 'follower', 'food', 'forever', 'forget', 'forward', 'four', 'france', 'free', 'freebie', 'french', 'fresh', 'friday', 'friend', 'fruit', 'fuck', 'full', 'fun', 'funny', 'future', 'g', 'gain', 'game', 'garden', 'get', 'gift', 'girl', 'give', 'giveaway', 'glad', 'glass', 'go', 'god', 'gold', 'gonna', 'good', 'goodbye', 'goodmorning', 'goodnight', 'google', 'gorgeous', 'gotta', 'great', 'group', 'grow', 'guess', 'guy', 'h', 'ha', 'haha', 'hahaha', 'hai', 'hair', 'half', 'happen', 'happiness', 'happy', 'hard', 'harry', 'hashtag', 'hate', \"he's\", 'head', 'hear', 'heart', 'hehe', 'hello', 'help', \"here's\", 'hey', 'hi', 'high', 'hit', 'hold', 'holiday', 'home', 'hop', 'hope', 'hopefully', 'hot', 'hotel', 'hour', 'house', 'hug', 'huge', 'human', 'hun', 'hurry', 'hurt', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ice', 'icecream', 'id', 'idea', 'ill', 'ily', 'im', 'image', 'include', 'incredible', 'indeed', 'indiemusic', 'influencer', 'influencers', 'info', 'inside', 'inspire', 'instagram', 'instead', 'interest', 'interested', 'interesting', 'international', 'interview', 'invite', 'island', 'issue', \"it'll\", 'james', 'jesus', 'job', 'john', 'join', 'journey', 'joy', 'july', 'jumma', 'junior', 'k', 'keep', 'khan', 'kid', 'kik', 'kikgirl', 'kikmeboys', 'kiksexting', 'kim', 'kind', 'kinda', 'kitchen', 'knock', 'know', 'kunorifor', 'l', 'la', 'lack', 'lady', 'last', 'late', 'later', 'latin', 'laugh', 'launch', 'lay', 'leader', 'league', 'learn', 'least', 'leave', 'less', 'let', \"let's\", 'liam', 'life', 'light', 'like', 'limit', 'line', 'link', 'list', 'listen', 'literally', 'little', 'live', 'lol', 'london', 'long', 'look', 'lose', 'loser', 'lot', 'love', 'lovely', 'low', 'luck', 'lucky', 'luke', 'lunch', 'luv', 'main', 'make', 'man', 'many', 'mary', 'masaan', 'match', 'mate', 'matt', 'matter', 'may', 'maybe', 'mean', 'medium', 'meet', 'meeting', 'mega', 'member', 'men', 'mention', 'message', 'michael', 'might', 'min', 'mind', 'mine', 'minecraft', 'minute', 'miss', 'mka', 'mm', 'model', 'mom', 'moment', 'monday', 'money', 'month', 'mood', 'morning', 'mother', 'move', 'movie', 'mr', 'mubarak', 'much', 'music', 'must', 'n', 'na', 'name', 'near', 'need', 'never', 'new', 'news', 'next', 'nice', 'night', 'nope', 'note', 'nothing', 'notice', 'number', 'offer', 'office', 'official', 'officially', 'oh', 'ok', 'okay', 'old', 'omg', 'one', 'online', 'oops', 'open', 'oppa', 'opportunity', 'order', 'original', 'others', 'otherwise', 'outside', 'p', 'pack', 'page', 'pain', 'panda', 'parent', 'paris', 'park', 'part', 'party', 'pass', 'past', 'pay', 'people', 'perfect', 'person', 'ph', 'phone', 'photo', 'pic', 'pick', 'picture', 'pin', 'place', 'plan', 'play', 'playlist', 'please', 'pleasse', 'pleasure', 'pls', 'plz', 'point', 'pool', 'pop', 'positive', 'possible', 'post', 'power', 'present', 'pretty', 'pride', 'pro', 'probably', 'problem', 'product', 'program', 'project', 'promise', 'proud', 'put', 'quacketyquack', 'queen', 'question', 'quick', 'quite', 'quote', 'r', 'race', 'radio', 'rain', 'rather', 'read', 'ready', 'real', 'realize', 'really', 'reason', 'receive', 'recent', 'recipe', 'record', 'red', 'regret', 'release', 'remember', 'reply', 'response', 'rest', 'retweet', 'retweeted', 'retweets', 'review', 'right', 'rise', 'road', 'rock', 'roll', 'room', 'rt', 'rts', 'ruin', 'run', 'sa', 'safe', 'saturday', 'save', 'saw', 'say', 'school', 'science', 'scope', 'season', 'second', 'secret', 'see', 'seem', 'selfie', 'selfies', 'send', 'sense', 'service', 'set', 'sex', 'sexy', 'shall', 'share', 'ship', 'shit', 'shoot', 'shop', 'short', 'shot', 'shout', 'show', 'side', 'sign', 'since', 'single', 'sir', 'sister', 'sit', 'site', 'skin', 'sleep', 'smile', 'snapchat', 'someone', 'something', 'sometimes', 'song', 'soon', 'sore', 'sorry', 'sort', 'sound', 'speak', 'special', 'spread', 'stand', 'star', 'start', 'stats', 'stay', 'stick', 'still', 'stop', 'store', 'story', 'stream', 'stress', 'strong', 'student', 'study', 'stuff', 'stupid', 'style', 'subject', 'success', 'suck', 'summer', 'sun', 'sunday', 'sunshine', 'super', 'support', 'suppose', 'sure', 'surprise', 'sweet', 'tag', 'take', 'talk', 'tank', 'tea', 'team', 'teen', 'tell', 'test', 'text', 'tgif', 'thank', 'thanks', 'thankyou', \"that's\", 'thats', \"there's\", \"they'll\", \"they're\", 'thing', 'think', 'thinking', 'tho', 'though', 'thought', 'three', 'throw', 'thx', 'ticket', 'till', 'time', 'today', 'together', 'tom', 'tomorrow', 'tonight', 'top', 'totally', 'touch', 'tour', 'train', 'travel', 'trip', 'true', 'trust', 'try', 'tuesday', 'tune', 'turn', 'tweet', 'twitch', 'twitter', 'two', 'ty', 'type', 'u', 'understand', 'unfollowers', 'update', 'ur', 'us', 'use', 'v', 'vacation', 'version', 'via', 'vid', 'video', 'view', 'visit', 'vote', 'w', 'wait', 'wake', 'walk', 'wanna', 'want', 'warm', 'warsaw', 'waste', 'watch', 'water', 'way', \"we'll\", \"we're\", \"we've\", 'wear', 'weather', 'website', 'week', 'week1', 'weekend', 'weight', 'welcome', 'well', \"what's\", 'whats', \"who's\", 'whole', 'whoop', 'wicked', 'win', 'window', 'wish', 'without', 'wolf', 'wonder', 'wonderful', 'word', 'work', 'workout', 'world', 'worry', 'worth', 'would', 'wow', 'write', 'wrong', 'wsale', 'x', 'xx', 'xxx', 'ya', 'yay', 'yeah', 'year', 'yep', 'yes', 'yesterday', 'yet', 'yo', 'young', 'youth', 'youtube', 'yup', '|', '~', '£', '–', '—', '’', '“', '”', '…', '←', '─', '♡', '♥', '✧', '❤', '️', '🍸', '👈', '👉', '💕', '💙', '💚', '💜', '😁', '😂', '😊', '😘']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predictTweet(tweet):\n",
        "  tweets = [tweet]\n",
        "  tweets = counter.transform(tweets)\n",
        "  return model.predict(tweets)\n",
        "\n",
        "def seePrediction(tweet):\n",
        "  if predictTweet(tweet)[0] == 1:\n",
        "    print('Your tweet was classified as Positive')\n",
        "  else:\n",
        "    print('Your tweet was classified as Negative')"
      ],
      "metadata": {
        "id": "-7aobRuBClGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positiveTweet1 = ''' \n",
        "        Just tried a new restaurant @bestrestauranthere and the food was absolutely amazing! Can't wait to go back again. 😊  #foodie #yum #delicious \n",
        "'''\n",
        "\n",
        "seePrediction(positiveTweet1)"
      ],
      "metadata": {
        "id": "WSx25Nr4CBVK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da19bb9-f998-4200-f95e-849092a20cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tweet was classified as Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### False Negative"
      ],
      "metadata": {
        "id": "3mdOoBrnOH55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positiveTweet2 = ''' \n",
        "        Can't believe it's been a year already! Time flies...💜 #anniversary\n",
        "'''\n",
        "\n",
        "seePrediction(positiveTweet2)"
      ],
      "metadata": {
        "id": "pTWAdhF_CSwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c05708-49f3-46e0-8576-54814db58e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tweet was classified as Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negativeTweet1 = ''' \n",
        "        It's amazing how one moment can change everything. Feeling heartbroken right now. ): #needtime\n",
        "'''\n",
        "\n",
        "seePrediction(negativeTweet1)"
      ],
      "metadata": {
        "id": "-dNCU68-CbbP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8331df4e-8645-469e-a34e-a051be44f1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tweet was classified as Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### False Positive"
      ],
      "metadata": {
        "id": "fL1zOCO8ONjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negativeTweet2 = ''' \n",
        "  This video made me think a lot https://www.youtube.com/ :d Can somebody talk to me? #feelingnotgood #needtotalk\n",
        "'''\n",
        "\n",
        "seePrediction(negativeTweet2)"
      ],
      "metadata": {
        "id": "Vnr3RoYoCfuv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69da8254-d805-449e-b8e6-1412f94b7ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your tweet was classified as Positive\n"
          ]
        }
      ]
    }
  ]
}